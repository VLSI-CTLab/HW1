{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW_1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zTvIwDlYvBzC"},"source":["# Initial Setup\n","\n","1. Import the CIFAR10 dataset.\n","2. Train a simple CNN (LeNet) to classify it."]},{"cell_type":"code","metadata":{"id":"hbiiMcdNJI--"},"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FG3WW1Owq1Fh"},"source":["**Note:** set the runtime type to \"GPU\" to run your code faster."]},{"cell_type":"code","metadata":{"id":"A3RF5VmcoUMG"},"source":["if torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nCaMDWYArEXO"},"source":["Load training and test data from the CIFAR10 dataset."]},{"cell_type":"code","metadata":{"id":"_5UuOjjrnogR"},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l62CkyIwtSOv"},"source":["Define a simple CNN that classifies CIFAR images."]},{"cell_type":"code","metadata":{"id":"9fL3F-7Rntog"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5, bias=False)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120, bias=False)\n","        self.fc2 = nn.Linear(120, 84, bias=False)\n","        self.fc3 = nn.Linear(84, 10, bias=False)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","net = Net().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nijieuxptag6"},"source":["Train this CNN on the training dataset (this may take a few moments)."]},{"cell_type":"code","metadata":{"id":"CzK6ohj5oNCT"},"source":["from torch.utils.data import DataLoader\n","\n","def train(model: nn.Module, dataloader: DataLoader):\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","    for epoch in range(2):  # loop over the dataset multiple times\n","\n","        running_loss = 0.0\n","        for i, data in enumerate(dataloader, 0):\n","            # get the inputs; data is a list of [inputs, labels]\n","            inputs, labels = data\n","\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            # zero the parameter gradients\n","            optimizer.zero_grad()\n","\n","            # forward + backward + optimize\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 2000 == 1999:    # print every 2000 mini-batches\n","                print('[%d, %5d] loss: %.3f' %\n","                    (epoch + 1, i + 1, running_loss / 2000))\n","                running_loss = 0.0\n","\n","    print('Finished Training')\n","\n","def test(model: nn.Module, dataloader: DataLoader, max_samples=None) -> float:\n","    correct = 0\n","    total = 0\n","    n_inferences = 0\n","\n","    with torch.no_grad():\n","        for data in dataloader:\n","            images, labels = data\n","\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","            if max_samples:\n","                n_inferences += images.shape[0]\n","                if n_inferences > max_samples:\n","                    break\n","    \n","    return 100 * correct / total"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HixhBHaqtmZU"},"source":["train(net, trainloader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJggxnCVuRxU"},"source":["Now that the CNN has been trained, let's test it on the test dataset."]},{"cell_type":"code","metadata":{"id":"y27_n-djuEdz"},"source":["score = test(net, testloader)\n","print('Accuracy of the network on the test images: {}%'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZVWbC5YWT-MU"},"source":["from copy import deepcopy\n","\n","# A convenience function which we use to copy CNNs\n","def copy_model(model: nn.Module) -> nn.Module:\n","    result = deepcopy(model)\n","\n","    # Copy over the extra metadata we've collected which copy.deepcopy doesn't capture\n","    if hasattr(model, 'input_activations'):\n","        result.input_activations = deepcopy(model.input_activations)\n","\n","    for result_layer, original_layer in zip(result.children(), model.children()):\n","        if isinstance(result_layer, nn.Conv2d) or isinstance(result_layer, nn.Linear):\n","            if hasattr(original_layer.weight, 'scale'):\n","                result_layer.weight.scale = deepcopy(original_layer.weight.scale)\n","            if hasattr(original_layer, 'activations'):\n","                result_layer.activations = deepcopy(original_layer.activations)\n","            if hasattr(original_layer, 'output_scale'):\n","                result_layer.output_scale = deepcopy(original_layer.output_scale)\n","\n","    return result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SQZoEjBSveV8"},"source":["# Question 1: Visualize Weights"]},{"cell_type":"code","metadata":{"id":"5qKRX7ply7I2"},"source":["import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2h7zJ8m3GAF"},"source":["# ADD YOUR CODE HERE to plot distributions of weights\n","\n","# You can get a flattened vector of the weights of fc1 like this:\n","#   fc1_weights = net.fc1.weight.data.cpu().view(-1)\n","# Try plotting a histogram of fc1_weights (and the weights of all the other layers as well)\n","conv1_weights = net.conv1.weight.data.cpu().view(-1)\n","conv2_weights = net.conv2.weight.data.cpu().view(-1)\n","fc1_weights = net.fc1.weight.data.cpu().view(-1)\n","fc2_weights = net.fc2.weight.data.cpu().view(-1)\n","fc3_weights = net.fc3.weight.data.cpu().view(-1)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2hKjshaHD11m"},"source":["# Question 2: Quantize Weights"]},{"cell_type":"code","metadata":{"id":"qXNk1fXuPGjB"},"source":["net_q2 = copy_model(net)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIBrqSFrVi5x"},"source":["from typing import Tuple\n","\n","def quantized_weights(weights: torch.Tensor) -> Tuple[torch.Tensor, float]:\n","    '''\n","    Quantize the weights so that all values are integers between -128 and 127.\n","    You may want to use the total range, 3-sigma range, or some other range when\n","    deciding just what factors to scale the float32 values by.\n","\n","    Parameters:\n","    weights (Tensor): The unquantized weights\n","\n","    Returns:\n","    (Tensor, float): A tuple with the following elements:\n","                        * The weights in quantized form, where every value is an integer between -128 and 127.\n","                          The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n","                        * The scaling factor that your weights were multiplied by.\n","                          This value does not need to be an 8-bit integer.\n","    '''\n","\n","    # ADD YOUR CODE HERE\n","\n","    # --------------------------------------------------------------------\n","    # Adopt the symmetric quantization which is easier.\n","    # - Two options can be applied: total range and 3-sigma range.\n","    # - Floating-point numbers can be mapped to 8bit integers [-128, 127]\n","    # --------------------------------------------------------------------\n","    \n","    scale = 2.5\n","    result = (weights * scale).round()\n","    return torch.clamp(result, min=-128, max=127), scale"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"orOwTnXxU1nb"},"source":["def quantize_layer_weights(model: nn.Module):\n","    for layer in model.children():\n","        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n","            q_layer_data, scale = quantized_weights(layer.weight.data)\n","            q_layer_data = q_layer_data.to(device)\n","\n","            layer.weight.data = q_layer_data\n","            layer.weight.scale = scale\n","\n","            if (q_layer_data < -128).any() or (q_layer_data > 127).any():\n","                raise Exception(\"Quantized weights of {} layer include values out of bounds for an 8-bit signed integer\".format(layer.__class__.__name__))\n","            if (q_layer_data != q_layer_data.round()).any():\n","                raise Exception(\"Quantized weights of {} layer include non-integer values\".format(layer.__class__.__name__))\n","\n","quantize_layer_weights(net_q2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wE3HqeBKVoYR"},"source":["score = test(net_q2, testloader)\n","print('Accuracy of the network after quantizing all weights: {}%'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xg7bfTF1bBVe"},"source":["# Question 3: Visualize Activations"]},{"cell_type":"code","metadata":{"id":"HP587b0QYxe9"},"source":["def register_activation_profiling_hooks(model: Net):\n","    model.input_activations = np.empty(0)\n","    model.conv1.activations = np.empty(0)\n","    model.conv2.activations = np.empty(0)\n","    model.fc1.activations = np.empty(0)\n","    model.fc2.activations = np.empty(0)\n","    model.fc3.activations = np.empty(0)\n","\n","    model.profile_activations = True\n","\n","    def conv1_activations_hook(layer, x, y):\n","        if model.profile_activations:\n","            model.input_activations = np.append(model.input_activations, x[0].cpu().view(-1))\n","    model.conv1.register_forward_hook(conv1_activations_hook)\n","\n","    def conv2_activations_hook(layer, x, y):\n","        if model.profile_activations:\n","            model.conv1.activations = np.append(model.conv1.activations, x[0].cpu().view(-1))\n","    model.conv2.register_forward_hook(conv2_activations_hook)\n","\n","    def fc1_activations_hook(layer, x, y):\n","        if model.profile_activations:\n","            model.conv2.activations = np.append(model.conv2.activations, x[0].cpu().view(-1))\n","    model.fc1.register_forward_hook(fc1_activations_hook)\n","\n","    def fc2_activations_hook(layer, x, y):\n","        if model.profile_activations:\n","            model.fc1.activations = np.append(model.fc1.activations, x[0].cpu().view(-1))\n","    model.fc2.register_forward_hook(fc2_activations_hook)\n","\n","    def fc3_activations_hook(layer, x, y):\n","        if model.profile_activations:\n","            model.fc2.activations = np.append(model.fc2.activations, x[0].cpu().view(-1))\n","            model.fc3.activations = np.append(model.fc3.activations, y[0].cpu().view(-1))\n","    model.fc3.register_forward_hook(fc3_activations_hook)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PVvPCIoabLC7"},"source":["net_q3 = copy_model(net)\n","register_activation_profiling_hooks(net_q3)\n","\n","# Run through the training dataset again while profiling the input and output activations this time\n","# We don't actually have to perform gradient descent for this, so we can use the \"test\" function\n","test(net_q3, trainloader, max_samples=400)\n","net_q3.profile_activations = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1HnYsuAMoxP"},"source":["input_activations = net_q3.input_activations\n","conv1_output_activations = net_q3.conv1.activations\n","conv2_output_activations = net_q3.conv2.activations\n","fc1_output_activations = net_q3.fc1.activations\n","fc2_output_activations = net_q3.fc2.activations\n","fc3_output_activations = net_q3.fc3.activations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEo8VK46bwjn"},"source":["# ADD YOUR CODE HERE to plot distributions of activations\n","\n","# Plot histograms of the following variables.  Calculate their ranges and 3-sigma ranges:\n","#   input_activations\n","#   conv1_output_activations\n","#   conv2_output_activations\n","#   fc1_output_activations\n","#   fc2_output_activations\n","#   fc3_output_activations\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"haiPVx4ibEra"},"source":["# Question 4: Quantize Activations"]},{"cell_type":"code","metadata":{"id":"zLjSp7hsXofq"},"source":["from typing import List\n","\n","class NetQuantized(nn.Module):\n","    def __init__(self, net_with_weights_quantized: nn.Module):\n","        super(NetQuantized, self).__init__()\n","        \n","        net_init = copy_model(net_with_weights_quantized)\n","\n","        self.conv1 = net_init.conv1\n","        self.pool = net_init.pool\n","        self.conv2 = net_init.conv2\n","        self.fc1 = net_init.fc1\n","        self.fc2 = net_init.fc2\n","        self.fc3 = net_init.fc3\n","\n","        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n","            def pre_hook(l, x):\n","                x = x[0]\n","                if (x < -128).any() or (x > 127).any():\n","                    raise Exception(\"Input to {} layer is out of bounds for an 8-bit signed integer\".format(l.__class__.__name__))\n","                if (x != x.round()).any():\n","                    raise Exception(\"Input to {} layer has non-integer values\".format(l.__class__.__name__))\n","\n","            layer.register_forward_pre_hook(pre_hook)\n","\n","        # Calculate the scaling factor for the initial input to the CNN\n","        self.input_activations = net_with_weights_quantized.input_activations\n","        self.input_scale = NetQuantized.quantize_initial_input(self.input_activations)\n","\n","        # Calculate the output scaling factors for all the layers of the CNN\n","        preceding_layer_scales = []\n","        for layer in self.conv1, self.conv2, self.fc1, self.fc2, self.fc3:\n","            layer.output_scale = NetQuantized.quantize_activations(layer.activations, layer.weight.scale, self.input_scale, preceding_layer_scales)\n","            preceding_layer_scales.append((layer.weight.scale, layer.output_scale))\n","\n","    @staticmethod\n","    def quantize_initial_input(pixels: np.ndarray) -> float:\n","        '''\n","        Calculate a scaling factor for the images that are input to the first layer of the CNN.\n","\n","        Parameters:\n","        pixels (ndarray): The values of all the pixels which were part of the input image during training\n","\n","        Returns:\n","        float: A scaling factor that the input should be multiplied by before being fed into the first layer.\n","               This value does not need to be an 8-bit integer.\n","        '''\n","\n","        # ADD YOUR CODE HERE\n","        # --------------------------------------------------------------------\n","        # Other than the first layer, we do need to quantize input feature maps \n","        # since they are the output feature maps from the previous layers.\n","        # --------------------------------------------------------------------\n","         \n","         \n","        # You may need to revise the following code to return correct values.\n","        return 1\n","\n","    @staticmethod\n","    def quantize_activations(activations: np.ndarray, n_w: float, n_initial_input: float, ns: List[Tuple[float, float]]) -> float:\n","        '''\n","        Calculate a scaling factor to multiply the output of a layer by.\n","\n","        Parameters:\n","        activations (ndarray): The values of all the pixels which have been output by this layer during training\n","        n_w (float): The scale by which the weights of this layer were multiplied as part of the \"quantize_weights\" function you wrote earlier\n","        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n","        ns ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n","\n","        Returns:\n","        float: A scaling factor that the layer output should be multiplied by before being fed into the first layer.\n","               This value does not need to be an 8-bit integer.\n","        '''\n","\n","        # ADD YOUR CODE HERE\n","        \n","        \n","        # You may need to revise the following code to return correct values.\n","        return 1\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        # You can access the output activation scales like this:\n","        #   fc1_output_scale = self.fc1.output_scale\n","\n","        # To make sure that the outputs of each layer are integers between -128 and 127, you may need to use the following functions:\n","        #   * torch.Tensor.round\n","        #   * torch.clamp\n","\n","        # ADD YOUR CODE HERE\n","\n","        \n","        \n","        # You may need to revise the following code to return correct values.\n","        return torch.ones(4,10).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"13CpHgvE994J"},"source":["# Merge the information from net_q2 and net_q3 together\n","net_init = copy_model(net_q2)\n","net_init.input_activations = deepcopy(net_q3.input_activations)\n","for layer_init, layer_q3 in zip(net_init.children(), net_q3.children()):\n","    if isinstance(layer_init, nn.Conv2d) or isinstance(layer_init, nn.Linear):\n","        layer_init.activations = deepcopy(layer_q3.activations)\n","\n","net_quantized = NetQuantized(net_init)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcBXEodN6hrY"},"source":["score = test(net_quantized, testloader)\n","print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1jTOL7scbMs7"},"source":["# Question 5: Quantize Biases"]},{"cell_type":"code","metadata":{"id":"bvv9-k1HPbgz"},"source":["class NetWithBias(nn.Module):\n","    def __init__(self):\n","        super(NetWithBias, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 6, 5, bias=False)\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv2 = nn.Conv2d(6, 16, 5, bias=False)\n","        self.fc1 = nn.Linear(16 * 5 * 5, 120, bias=False)\n","        self.fc2 = nn.Linear(120, 84, bias=False)\n","        self.fc3 = nn.Linear(84, 10, bias=True)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = x.view(-1, 16 * 5 * 5)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","net_with_bias = NetWithBias().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wjk3hEQaVDpq"},"source":["train(net_with_bias, trainloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9vLUCDnnVf4R"},"source":["score = test(net_with_bias, testloader)\n","print('Accuracy of the network (with a bias) on the test images: {}%'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U_ZiJk6yEEM-"},"source":["register_activation_profiling_hooks(net_with_bias)\n","test(net_with_bias, trainloader, max_samples=400)\n","net_with_bias.profile_activations = False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZwk8KLtAUAM"},"source":["net_with_bias_with_quantized_weights = copy_model(net_with_bias)\n","quantize_layer_weights(net_with_bias_with_quantized_weights)\n","\n","score = test(net_with_bias_with_quantized_weights, testloader)\n","print('Accuracy of the network on the test images after all the weights are quantized but the bias isn\\'t: {}%'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mO2Gdu_tEZ4v"},"source":["class NetQuantizedWithBias(NetQuantized):\n","    def __init__(self, net_with_weights_quantized: nn.Module):\n","        super(NetQuantizedWithBias, self).__init__(net_with_weights_quantized)\n","\n","        preceding_scales = [(layer.weight.scale, layer.output_scale) for layer in self.children() if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear)][:-1]\n","\n","        self.fc3.bias.data = NetQuantizedWithBias.quantized_bias(\n","            self.fc3.bias.data,\n","            self.fc3.weight.scale,\n","            self.input_scale,\n","            preceding_scales\n","        )\n","\n","        if (self.fc3.bias.data < -2147483648).any() or (self.fc3.bias.data > 2147483647).any():\n","            raise Exception(\"Bias has values which are out of bounds for an 32-bit signed integer\")\n","        if (self.fc3.bias.data != self.fc3.bias.data.round()).any():\n","            raise Exception(\"Bias has non-integer values\")\n","\n","    @staticmethod\n","    def quantized_bias(bias: torch.Tensor, n_w: float, n_initial_input: float, ns: List[Tuple[float, float]]) -> torch.Tensor:\n","        '''\n","        Quantize the bias so that all values are integers between -2147483648 and 2147483647.\n","\n","        Parameters:\n","        bias (Tensor): The floating point values of the bias\n","        n_w (float): The scale by which the weights of this layer were multiplied\n","        n_initial_input (float): The scale by which the initial input to the neural network was multiplied\n","        ns ([(float, float)]): A list of tuples, where each tuple represents the \"weight scale\" and \"output scale\" (in that order) for every preceding layer\n","\n","        Returns:\n","        Tensor: The bias in quantized form, where every value is an integer between -2147483648 and 2147483647.\n","                The \"dtype\" will still be \"float\", but the values themselves should all be integers.\n","        '''\n","\n","        # ADD YOUR CODE HERE\n","        \n","\n","        \n","        return torch.clamp((bias * 2.5).round(), min=-2147483648, max=2147483647)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TA6rXt3Q-zF8"},"source":["net_quantized_with_bias = NetQuantizedWithBias(net_with_bias_with_quantized_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJvR6Wv_GJJX"},"source":["score = test(net_quantized_with_bias, testloader)\n","print('Accuracy of the network on the test images after all the weights and the bias are quantized: {}%'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U_W-RCmHjlkg"},"source":["# Extract the quantized CNN model\r\n","\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"AllumB6-YI9B"},"source":["# Copy the quantized model with bias and save the weights\r\n","inference_model = copy_model(net_quantized_with_bias)\r\n","torch.save(inference_model.state_dict(), 'net_quantized_with_bias.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWPqoj8tf0Rr"},"source":["# Check again to make sure it has the same accuray\r\n","score = test(inference_model, testloader)\r\n","print('Accuracy of the network after quantizing both weights and activations: {}%'.format(score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5TfpcnC60_g4"},"source":["Use a random image as an input of the activations"]},{"cell_type":"code","metadata":{"id":"CByZgw-ftMBD"},"source":["# Randomly choose an image as the input and get the output\r\n","index = np.random.randint(0,10000)\r\n","input, label = testset[index]\r\n","output = inference_model(input.unsqueeze(0).cuda())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x9M4QXt60WmL"},"source":["Save the input/output activations to csv"]},{"cell_type":"code","metadata":{"id":"WSIdYFyZshFj"},"source":["import os \r\n","import zipfile\r\n","# It is easier to download all the files with zip\r\n","zf = zipfile.ZipFile('parameters.zip', 'w', zipfile.ZIP_DEFLATED)\r\n","\r\n","if not os.path.exists('./activations'):\r\n","    os.mkdir('./activations')\r\n","\r\n","np.savetxt('./activations/input.csv', input.cpu().data.numpy().reshape(-1), delimiter=',')\r\n","np.savetxt('./activations/output.csv', output.cpu().data.numpy().reshape(-1), delimiter=',')\r\n","zf.write('./activations/input.csv')\r\n","zf.write('./activations/output.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5jo_TlzE1dqy"},"source":["Save the weights of each layer to CSV format"]},{"cell_type":"code","metadata":{"id":"27MD_hJUzlnf"},"source":["if not os.path.exists('./weights'):\r\n","    os.mkdir('./weights')\r\n","    \r\n","for name, weights in inference_model.state_dict().items():\r\n","    print(name, 'with shape:' , weights.shape)\r\n","    np.savetxt('./weights/%s.csv' %(name) , weights.cpu().reshape(-1), delimiter=',')\r\n","    zf.write('./weights/%s.csv' %(name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mG9TAca-Jbg6"},"source":["Record the sacling factors to json file"]},{"cell_type":"code","metadata":{"id":"VNqV21YVimKD"},"source":["import json\r\n","scales = {'input_scale':inference_model.input_scale, 'conv1_output_scale': inference_model.conv1.output_scale, 'conv2_output_scale':inference_model.conv2.output_scale,\r\n","        'fc1_output_scale' :inference_model.fc1.output_scale, 'fc2_output_scale':inference_model.fc2.output_scale, 'fc3_output_scale':inference_model.fc3.output_scale}\r\n","\r\n","with open('scale.json', 'w', newline='') as jsonfile:\r\n","    json.dump(scales, jsonfile)\r\n","\r\n","zf.write('./scale.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HbUYA1VpKLAB"},"source":["Close the zip file"]},{"cell_type":"code","metadata":{"id":"BlOLRLdTJqcm"},"source":["zf.close()\r\n","# Files are located on left-hand-side menubar.\r\n","# You can downlaod them now. \r\n","# Enjoy!"],"execution_count":null,"outputs":[]}]}